% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mELO.R
\name{mELO}
\alias{mELO}
\title{mELO Ratings}
\usage{
mELO(
  match_data,
  init_data = NULL,
  init_c_mat = NULL,
  init_rating = 2200,
  k = ifelse(!is.null(init_c_mat), ncol(as.matrix(init_c_mat)), 1),
  eta_1 = 27,
  eta_2 = 1,
  p1_advantage = 0,
  save_history = TRUE,
  sort = TRUE
)
}
\arguments{
\item{match_data}{A data frame containing four columns: (1) a numeric
vector denoting the time period in which the game took place (2) a numeric
or character identifier for player one (3) a numeric or character identifier
for player two and (4) the result of the game expressed as a number,
typically equal to one for a player one win, zero for a player two win and
one half for a draw.}

\item{init_data}{Initialise a rating model with \code{ratings} component of the
returned list of the output from a previous \code{mELO()} call, or a data.frame with
columns Player and Rating.}

\item{init_c_mat}{Initialise a rating model with \code{c_mat} component of the
returned list of the output from a previous \code{mELO()} call, or a matrix with
with dimensions (#agents, k). Recall the C matrix encodes the non-transitive
interactions of the agents. This is initialised randomly if not provided.}

\item{init_rating}{The initial rating for players not appearing in
\code{init_data}.}

\item{k}{Integer defining the complexity of non-transitive interactions to model.}

\item{eta_1}{Learning rate for the ratings vector.}

\item{eta_2}{Learning rate for elements of the C matrix.}

\item{p1_advantage}{Player 1 advtange parameter. Either a single value or a
vector equal to the number of rows in \code{match_data}.}

\item{save_history}{If \code{TRUE} return the rating history for each player.}

\item{sort}{If \code{TRUE}, sort the output ratings from highest to lowest.}
}
\value{
A list object of class "\code{mELO_rating}" with the following
components:
\describe{
  \item{ratings}{A data frame of the results at the end of the final time period.
  The variables are self explanatory except for Lag, which represents the
  number of time periods since the player last played a game. This is equal
  to zero for players who played in the latest time period, and is also zero
  for players who have not yet played any games.}
  \item{history}{An array containing the ratings history for all
  players.}
  \item{c_mat}{An estimate of the C matrix.}
  \item{c_mat_history}{An array containing the history of the C matrix.}
  \item{p1_advantage}{A single value or a vector values for the
  advantage Player 1 had.}
  \item{k}{Integer defining the complexity of non-transitive interactions to model.}
  \item{eta_1}{Learning rate for the ratings vector.}
  \item{eta_2}{Learning rate for elements of the C matrix.}
  \item{type}{The type of model. In this case, "mELO".}
  \item{preds}{The player 1 success probabilities predicted prior to adjusting to the
  outcome of the match.}
  \item{outcomes}{The outcome for player 1 for each match.}
  \item{preds_logloss}{The mean logloss error for all predictions.}

}
}
\description{
This function implements the multidimensional ELO (mELO) rating system for
performing pairwise comparisons. The mELO rating system has the desirable
properties of being able to handle cyclic interactions and is better behaved
in the presence of redundant copies of players/agents or tasks.
}
\examples{
# Rock paper scissors
head(rps_df)

# Note that ELO doesn't perform well
rps_ELO <- ELO(rps_df)
rps_ELO
ELO_preds <- predict(
    rps_ELO,
    head(rps_df)
)
cbind(
    head(rps_df),
    ELO_preds
)
    # Predictions are all ~0.5

# Fit a mELO model that can handle these types of interactions.
rps_mELO <- mELO(rps_df, k=1)
rps_mELO
# Inspect advantage matrix
get_adv_mat(rps_mELO)
# Get predictions
mELO_preds <- predict(
    rps_mELO,
    head(rps_df)
)
cbind(
    head(rps_df),
    mELO_preds
)
    # Much better predictions!
}
\references{
Balduzzi, David, et al. "Re-evaluating Evaluation." arXiv preprint arXiv:1806.02643 (2018).
}
