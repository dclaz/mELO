---
title: "Application of mELO to AFL matches"
author: "David Lazaridis"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
    toc: true
vignette: >
  %\VignetteIndexEntry{Application of mELO ratings to AFL matches}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Introduction

As demonstrated in the [introductory vignette](introduction.html), Elo and 
other methods that rely on a single parameter that describe an agents ability 
cannot accomodate scenarios with cyclical or non-transitive interactions amongst 
agents or players. In situations where these behaviours are expected, a mELO 
model is likely to achieve far better results than an Elo model provided the 
outcomes aren't too noisy and the agents abilities are locally stationary.

**Note:** *Stationarity is important*. If agents *true* abilities change too fast, 
statistical methods are not likely to yield useful estimates or predictions.

In this document we will use the **mELO** package to see if a mELO model can 
outperform a regular Elo model for the task of predicting the outcome of [AFL](https://en.wikipedia.org/wiki/Australian_Football_League) matches. We do not
expect the performance of the mELO model to be amazing. Other methods for predicting 
match outcomes that are more highly parameterised (such as Glicko or Steph as implemented in the 
[PlayerRating](https://cran.r-project.org/package=PlayerRatings) package) or are 
able to make predictions conditional on additional information (other statistical 
or machine learning methods) would likely yield better results.


# Method

We will apply the following procedure to build models and assess their 
performance:

1. **Split data in to the following segments**
    * **Initialisation data**. Seasons 2012--2014 inclusive.
    * **Training data**. Seasons 2015--2018 inclusive.
    * **Test data**. Seasons 2019.
2. **Model fitting**
    * Select hyperparamters: learning rates & home ground advantage.
    * Fit model to initialisation data
    * Fit model to training data, initialised with the estimates obtained from 
    the initialised model.
3. **Model optimisation** 
    * Repeat step 2 to find hyperparamters that minimises the chosen loss f
    unction on the training data.
4. **Get final estimate of model error**
    * Using the optimal model from step 3, calculate error on the test data.

The initialisation step is necessary because the initial ratings for all agents will be 
set to the same value and the $\textbf{C}$ matrix will have been initialised randomly. 
The first segment of the AFL match data is used to initialise the model so that 
reasonable estimates of $\textbf{r}$ and $\textbf{C}$ can be obtained and plugged in 
to the training model before we start calculating estimated model errors. 

The outcomes we are modelling will be $y \in {0, 0.5, 1}$ representing a loss, 
draw and win respectively.

We will use the logarithmic loss function (equivalent to cross-entropy in this setting) to evaluate model fits, calculated as
$$ \mathcal{L(\Theta)} = - \frac{1}{n} \sum_{i=1}^n [y_{i} \log \, p_{i} + (1 - y_{i}) \log \, (1 - p_{i})].$$
where $i$ is the match index, $n$ is the total number of matches we are 
calculating the loss over and $\Theta$ is the set of model hyperparamters.

**Note:** Alternatively for step 2, because these are iterative procedures we can fit a model with the specified hyperparameters to the initialisation data + training data, and then in step 3 we only count the errors from the training data portion.


# The data

A large sample of AFL match data has been sourced using the excellent [fitzRoy](https://cran.r-project.org/package=fitzRoy) package and made 
available in the `afl_df` object.

```{r setup}
# Load essential packages
library(mELO)
library(dplyr)

# Inspect AFL match data
head(afl_df) %>% knitr::kable()
```

Split the data in to the required segments

```{r data_setep}
# Initialisation data
init_df <- afl_df %>%
  filter(
    date > as.Date("2012-01-01"),
    date < as.Date("2014-01-01")
  ) %>%
  select(match_index, home_team, away_team, outcome)

# Training data
train_df <- afl_df %>%
  filter(
    date > as.Date("2014-01-01"),
    date < as.Date("2018-01-01")
  ) %>%
  select(match_index, home_team, away_team, outcome)

# Test data
test_df <- afl_df %>%
  filter(
    date > as.Date("2018-01-01"),
    date < as.Date("2020-01-01")
  ) %>%
  select(match_index, home_team, away_team, outcome)

# Combine init and train for convenience (See note above)
init_train_df <- bind_rows(
  init_df,
  train_df
)

# How many games in each data set
n_init <- nrow(init_df)
n_train <- nrow(train_df)
n_test <- nrow(test_df)

```

# The models

For the sake of simplicity we will apply the homeground advantage to 
all teams playing at home. A more rigorous experiment would be to identify in 
which matches the away team had a travel disadvantage, and then set the home team
to have an advantage in that match. 


## ELO

We begin by writing a function to help us perform step 2. This function 
returns the logloss over the training data for an Elo model fit with the specified
hyperparameters, $\Theta = (\eta, \gamma)$, the learning rate and home ground 
advantage respectively. 

**Note:** The `ELO()` and `mELO()` functions return the true outcomes
and the predictions for time $t+1$ made at time $t$ which make the loss calculations 
straightforward and removes the requirement to write a function to loop through matches.

```{r ELO_error_fn}
# Function to return logloss for current selection of hyperparameters
ELO_logloss_error <- function(
  hyperparams
){
  # Fit model to INIT + TRAIN data
  model <- ELO(
    init_train_df,
    eta = hyperparams[1],          # \eta
    p1_advantage = hyperparams[2], # \gamma
    save_history = FALSE
  )
  
  # Calculate loss on the TRAIN data
  # Note these predictions are made before the ratings are updated 
  train_logloss <- logloss(
    tail(model$preds, n_train),
    tail(model$outcomes, n_train)
  )
  
  return(train_logloss)
}

```

Now we can use an optimisation procedure to find the hyperparameters $\Theta$ 
such that the logloss error is minimised. 


```{r ELO_optimisation}
# Optimisation (assuming it's smooth)
ELO_best_params <- optim(
  c(30,100),
  ELO_logloss_error,
  method = "BFGS"
  
)

ELO_best_params

```

The best model is obtained when $\eta=$ `r round(ELO_best_params$par[1],2)` and 
$\gamma=$ `r round(ELO_best_params$par[2],2)` (where the logarithmic loss $\mathcal{L}=$ `r round(ELO_best_params$value,2)`). Now we can fit a model with these parameters and calculate the error on the test set.

```{r ELO_test}
# Fit model with optimal hyperparams
optim_ELO_model <- ELO(
  bind_rows(
    init_train_df,
    test_df
  ),
  eta = ELO_best_params$par[1],          # \eta
  p1_advantage = ELO_best_params$par[2], # \gamma
)

# Generate predictions on test set
test_preds_ELO <- tail(optim_ELO_model$preds, n_test)

# Calculate logloss error on test set
test_logloss_ELO <- logloss(
  test_preds_ELO,
  test_df$outcome
)

# Calculate classification rate on test set
test_class_rate_ELO <- mean(
  (as.numeric(test_preds_ELO >= 0.5) & test_df$outcome == 1) |
    (as.numeric(test_preds_ELO < 0.5) & test_df$outcome == 0)
)
```
Results:

 * Test logloss $\mathcal{L}=$ `r test_logloss`.
 * Test classification rate = `r round(test_class_rate*100, 1)`%.

The figure below gives the evolutions of the Elo ratings. The vertical lines indicate the cut offs of the initialisation, training and test sets.

```{r ELO_plot}
plot(optim_ELO_model)
abline(v=c(n_init, n_init+n_train), lty=2)
```


## mELO

Similarly to the Elo model, we build a function to help us optimise the loss function.

```{r mELO_error_fn}
# Function to return logloss for current selection of hyperparameters
# k = 1
mELO_logloss_error <- function(
  hyperparams
){
  # Set seed, C matrix is initialised randomly
  set.seed(1337)
  
  # Fit model to INIT + TRAIN data
  model <- mELO(
    init_train_df,
    # init_c_mat = 
    k = 2,
    eta_1 = hyperparams[1],          # \eta_1
    eta_2 = hyperparams[2],          # \eta_2
    p1_advantage = hyperparams[3],   # \gamma
    save_history = FALSE
  )
  
  # Calculate loss on the TRAIN data
  # Note these predictions are made before the ratings are updated 
  train_logloss <- logloss(
    tail(model$preds, n_train),
    tail(model$outcomes, n_train)
  )
  
  return(train_logloss)
}

```
Now we can use an optimisation procedure to find the hyperparameters $\Theta$ 
such that the logloss error is minimised. 

```{r mELO_optimisation}
# Optimisation (assuming the surface is reasonably well behaved)
mELO_best_params <- optim(
  c(20, 0.1, 75),
  mELO_logloss_error,
  method = "L-BFGS-B",
  lower = c(5, 0.01, 40),
  upper = c(50, 2, 120)
)

mELO_best_params

```

The best model is obtained when $\eta_1=$ `r round(mELO_best_params$par[1],2)`, 
$\eta_2=$ `r round(mELO_best_params$par[2],2)` and 
$\gamma=$ `r round(mELO_best_params$par[3],2)` (where the logarithmic loss $\mathcal{L}=$ `r round(mELO_best_params$value,2)`). Now we can fit a model with these parameters and calculate the error on the test set.


```{r mELO_test}
# Fit model with optimal hyperparams
optim_mELO_model <- mELO(
  bind_rows(
    init_train_df,
    test_df
  ),
  k = 1,
  eta_1 = mELO_best_params$par[1], 
  eta_2 = mELO_best_params$par[2],
  p1_advantage = mELO_best_params$par[3], 
)

# Generate predictions on test set
test_preds_mELO <- tail(optim_mELO_model$preds, n_test)

# Calculate logloss error on test set
test_logloss_mELO <- logloss(
  test_preds_mELO,
  test_df$outcome
)

# Calculate classification rate on test set
test_class_rate_mELO <- mean(
  (as.numeric(test_preds_mELO >= 0.5) & test_df$outcome == 1) |
    (as.numeric(test_preds_mELO < 0.5) & test_df$outcome == 0)
)
```
Results:

 * logloss $\mathcal{L}=$ `r test_logloss`.
 * Classification rate = `r round(test_class_rate*100, 1)`%.

The figure below gives the evoltions of the Elo ratings. The vertical lines indicate the cut offs of the initialisation, training and test sets.

```{r mELO_plot}
plot(optim_mELO_model)
abline(v=c(n_init, n_init+n_train), lty=2)

```

We can also inspect the advantage matrix and plot the $\textbf{c}$ vectors for each team.

```{r mELO_plot}
# Get adv matrix
#get_adv_mat(optim_mELO_model)
# Plot c vectors for each team
plot_c_mat(optim_mELO_model)

```
# Conclusion

